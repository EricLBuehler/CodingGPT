{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHZnXywjxGeq"
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY_j8RUDP5y4"
   },
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "from torch.autograd.variable import Variable\n",
    "import typing\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4lRdOSZNpe2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ph2WO5JpKukk",
    "outputId": "ab00d895-307a-4ade-e705-1c6dd8ae6926"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modelname=\"5_3_23_m1\"\n",
    "\n",
    "prefix_models=\"models/\"+modelname+\"/\"\n",
    "\n",
    "if not os.path.exists(prefix_models):\n",
    "    os.makedirs(prefix_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe3wtt5oADl6"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGOoJdNdRSaO"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        #mine is [batch, seq, embed]\n",
    "        x = x.permute((1,0,2))\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        dropout = self.dropout(x)\n",
    "        return dropout.permute((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XakHZ4HfFdYM"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        key_tp = key.transpose(-2, -1)\n",
    "\n",
    "        scores = query.matmul(key_tp) / math.sqrt(query.size()[-1])\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            \n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "\n",
    "        return attention.matmul(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-sb6gd7L5bR"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 head_num,\n",
    "                 bias=True,\n",
    "                 activation=F.relu):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if in_features % head_num != 0:\n",
    "            raise ValueError('`in_features`({}) should be divisible by \\\n",
    "                `head_num`({})'.format(in_features, head_num))\n",
    "        self.in_features = in_features\n",
    "        self.head_num = head_num\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "        self.linear_q = nn.Linear(in_features, in_features, bias)\n",
    "        self.linear_k = nn.Linear(in_features, in_features, bias)\n",
    "        self.linear_v = nn.Linear(in_features, in_features, bias)\n",
    "        self.linear_o = nn.Linear(in_features, in_features, bias)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)\n",
    "        if self.activation is not None:\n",
    "            q = self.activation(q)\n",
    "            k = self.activation(k)\n",
    "            v = self.activation(v)\n",
    "\n",
    "        q = self._reshape_to_batches(q)\n",
    "        k = self._reshape_to_batches(k)\n",
    "        v = self._reshape_to_batches(v)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(self.head_num, 1, 1)   \n",
    "        \n",
    "        y = ScaledDotProductAttention()(q, k, v, mask)        \n",
    "        \n",
    "        y = self._reshape_from_batches(y)      \n",
    "\n",
    "        y = self.linear_o(y)\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_causal_mask(x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        return torch.tril(torch.ones(seq_len, seq_len)).view(1, seq_len, seq_len).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def _reshape_to_batches(self, x):\n",
    "        batch_size, seq_len, in_feature = x.size()\n",
    "        sub_dim = in_feature // self.head_num\n",
    "        return x.reshape(batch_size, seq_len, self.head_num, sub_dim)\\\n",
    "                .permute(0, 2, 1, 3)\\\n",
    "                .reshape(batch_size * self.head_num, seq_len, sub_dim)\n",
    "\n",
    "    def _reshape_from_batches(self, x):\n",
    "        batch_size, seq_len, in_feature = x.size()\n",
    "        batch_size //= self.head_num\n",
    "        out_dim = in_feature * self.head_num\n",
    "        return x.reshape(batch_size, self.head_num, seq_len, in_feature)\\\n",
    "                .permute(0, 2, 1, 3)\\\n",
    "                .reshape(batch_size, seq_len, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDpTASiaTxTX"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int,\n",
    "                 n_self_heads: int,\n",
    "                 n_features: int,\n",
    "                 n_layers: int,\n",
    "                 n_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #Embedding layer\n",
    "        self.embedding = nn.Embedding(n_features, embedding_dim)\n",
    "        #Positional encoding\n",
    "        self.pos_encode = PositionalEncoding(embedding_dim)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            layer = []\n",
    "            #Add multihead, which will be cross or self attention\n",
    "            layer.append(MultiHeadAttention(embedding_dim, n_self_heads)) #self attention first, masked\n",
    "            #Now add layer norm\n",
    "            layer.append(nn.LayerNorm(embedding_dim))\n",
    "            #Add a feed forward\n",
    "            layer.append(nn.Linear(embedding_dim, embedding_dim))\n",
    "            #Now add layer norm\n",
    "            layer.append(nn.LayerNorm(embedding_dim))\n",
    "\n",
    "            self.decoder_layers.append(nn.ModuleList(layer))\n",
    "        self.decoder_layers=nn.ModuleList(self.decoder_layers)\n",
    "\n",
    "        self.to_out = nn.Linear(embedding_dim, n_classes)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, calculate_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Expect tensor of [batch_size, n_features]\n",
    "        \"\"\"\n",
    "        if calculate_loss:\n",
    "            #If give model that accepts ?x?x4 abcd, expect bcd0\n",
    "            \n",
    "            target_logits=torch.cat([x[:,1:], torch.zeros((x.shape[0],1)).to(device)], dim=-1) ## if x is abcd, then target_logits is bcd0\n",
    "\n",
    "        x=x.long().to(device)\n",
    "        embed = self.embedding(x)\n",
    "        pos_encode = self.pos_encode(embed)\n",
    "\n",
    "        res = embed+pos_encode\n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            d_self_attention = decoder_layer[0]\n",
    "            d_layer_norm_1 = decoder_layer[1]\n",
    "            d_ff = decoder_layer[2]\n",
    "            d_layer_norm_2 = decoder_layer[3]\n",
    "            \n",
    "            ## Run the decoder\n",
    "            #do masked self attention\n",
    "            mask = MultiHeadAttention.gen_causal_mask(res).to(device)\n",
    "            res = res + d_self_attention(res,res,res, mask = mask)\n",
    "            self_res = res\n",
    "            #layer norm\n",
    "            res = d_layer_norm_1(res)\n",
    "\n",
    "            #do ff\n",
    "            res = self_res + d_ff(res)\n",
    "            #layer norm\n",
    "            res = d_layer_norm_2(res)\n",
    "\n",
    "        out = self.to_out(res)\n",
    "        if calculate_loss:\n",
    "            loss = nn.functional.cross_entropy(out.permute(0, 2, 1), target_logits.long())\n",
    "            return out,loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqGGEpvCLbP8"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, n_pad: int, device: torch.device, pad_byte: int = 0, split: str = \"\\n\"):\n",
    "        self.n_pad = n_pad\n",
    "        self.device = device\n",
    "        self.pad_byte = pad_byte\n",
    "        self.split = split\n",
    "\n",
    "    def tokenize_str(self, sentence: str, encoding = \"utf8\") -> torch.Tensor:\n",
    "        base = [int(i) for i in bytes(sentence, encoding)]\n",
    "        if len(base) < self.n_pad:\n",
    "            base.extend([self.pad_byte] * (self.n_pad - len(base)))\n",
    "        assert len(base) == self.n_pad, f\"n_pad is too small, use {len(base)} or greater.\"\n",
    "        tensor = torch.Tensor(base)\n",
    "        return tensor.to(self.device)\n",
    "\n",
    "    def texts_to_sequences(self, texts: typing.List[str], encoding = \"utf8\") -> torch.Tensor:\n",
    "        # tokenize the input text\n",
    "        sentences = []\n",
    "        for sentence in texts:\n",
    "            sentences.append(self.tokenize_str(sentence).unsqueeze(0))\n",
    "\n",
    "        return torch.cat(sentences, dim = 0).to(self.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_texts(document: str) -> typing.List[str]:\n",
    "        return filter(lambda x: len(x)!=0, document.split(self.split))\n",
    "    \n",
    "    def sequences_to_texts(self, texts: torch.Tensor, encoding = \"utf8\") -> typing.List[str]:\n",
    "        out = []\n",
    "        for seq in texts:\n",
    "            chars = []\n",
    "            i=0\n",
    "            while i<len(seq) and seq[i] != 0:\n",
    "                chars.append(int(seq[i]))\n",
    "                i+=1\n",
    "            out.append(bytes(chars).decode(encoding, \"replace\"))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB-Xcf4TkKNQ"
   },
   "outputs": [],
   "source": [
    "def generate(seed: str, cutoff: int = 1024) -> str:\n",
    "    output = torch.tensor([list(bytes(seed,\"utf8\"))]).to(device)\n",
    "    \n",
    "    res=output\n",
    "    last = -1\n",
    "    i=0\n",
    "    while last != 0 and i<cutoff:\n",
    "        res = model(output)\n",
    "        argmax=res.argmax(-1)\n",
    "        \n",
    "        out = list(output[0])\n",
    "        out.append(list(argmax.to(device)[0])[-1])\n",
    "        last = list(argmax.to(device)[0])[-1]\n",
    "        output = torch.tensor([out])\n",
    "        i+=1\n",
    "    \n",
    "    if last == 0:\n",
    "        return convert_to_str(output)\n",
    "    return convert_to_str(output)+\"<CUTOFF>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R81mAag8mHEG"
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: typing.List[str], n_pad):\n",
    "        self.raw_data = data\n",
    "        self.tokenizer = Tokenizer(n_pad, device, split = \"\\0\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.tokenizer.tokenize_str(self.raw_data[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQidsLjVaFUi"
   },
   "outputs": [],
   "source": [
    "def convert_to_str(x: torch.Tensor) -> str:\n",
    "    #Expects [1, 256] tensor\n",
    "    bts = []\n",
    "    i=0\n",
    "    while len(bts)<x.shape[1] and x[0][i] != 0:\n",
    "        bts.append(int(x[0][i]))\n",
    "        i+=1\n",
    "    return bytes(bts).decode(\"utf8\",\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97rbDfS4gO8C"
   },
   "outputs": [],
   "source": [
    "n_features = 256 # No. of tokens\n",
    "n_pad = 512 # Max line length\n",
    "embedding_dim = 640\n",
    "batch_size = 32\n",
    "head_factor = 64\n",
    "assert embedding_dim%head_factor == 0\n",
    "head_size = embedding_dim//head_factor\n",
    "print(f\"Head size: {head_size}\")\n",
    "n_layers = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvvenMeUKfaj",
    "outputId": "fb38b6f5-0eaf-4d89-83ad-bee41604b8ef"
   },
   "outputs": [],
   "source": [
    "path_to_data = \"data/reddit_scrape_v8.txt\"\n",
    "data_raw = open(path_to_data, encoding=\"utf-8\").read()\n",
    "\n",
    "data_split = list(filter(lambda x: x!=\"\", data_raw.split(\"\\0\")))\n",
    "random.shuffle(data_split)\n",
    "\n",
    "train_data = data_split[100:]\n",
    "print(len(train_data))\n",
    "val_data = data_split[:100]\n",
    "\n",
    "train_dataloader = TextDataset(train_data, n_pad)\n",
    "test_dataloader = TextDataset(train_data, n_pad)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataloader, batch_size=batch_size)\n",
    "testloader = torch.utils.data.DataLoader(test_dataloader , batch_size=1)\n",
    "testloader_iter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvibftHanb0A"
   },
   "outputs": [],
   "source": [
    "model = Transformer(embedding_dim, head_size, n_features, n_layers, 256)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model), \"trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXqmUSK0M3hD",
    "outputId": "69ad8160-d138-4e98-99b6-bd8cb83753c1"
   },
   "outputs": [],
   "source": [
    "input=next(testloader_iter)\n",
    "input=input.to(device)\n",
    "print(input.shape)\n",
    "res = model(input)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZji9Va7-unn"
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUN0DJQo-vmf"
   },
   "outputs": [],
   "source": [
    "n_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GczR7J4u6L5Q",
    "outputId": "15259e58-4468-44ff-dcf3-6c8dfb86d6bc"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYr65bmbITE1"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Why does Earth orbit the Sun?\",\n",
    "    \"Hello world, \",\n",
    "    \"How to use ChatGPT?\",\n",
    "    \"My code does not work. What should I do?\",\n",
    "    \"Why is this code not working: `1+\\\"A\\\"`?\",\n",
    "    \"Why is Java better than Python?\",\n",
    "    \"Why is Python better than Java?\",\n",
    "    \"What is the purpose of the main() function in C?\",\n",
    "    \"What is coding?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o85FrKyF-wg-"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    batch_losses = []\n",
    "    for data in tqdm.tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        output, loss = model(data, True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        \n",
    "    epoch_losses.append(sum(batch_losses)/len(batch_losses))\n",
    "\n",
    "    plt.plot(range(len(batch_losses)),batch_losses)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Batch loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(len(epoch_losses)), epoch_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Epoch loss\")\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(model, prefix_models+f\"model_E{epoch}\")\n",
    "\n",
    "    with open(prefix_models+\"losses.txt\", \"a\") as f:\n",
    "        f.write(f\"{epoch_losses[-1]}\\n\")\n",
    "        \n",
    "    scheduler.step()    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"Prompt {i}: {prompt}\")\n",
    "            output=generate(prompt)\n",
    "            print(f\"Model output: {output}\")\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUracnfcZSLX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(prefix_models+\"model_E0\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        output=generate(prompt)\n",
    "        print(f\"Model output: {output}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r31WSN9YiTuZ"
   },
   "outputs": [],
   "source": [
    "import builtins\n",
    "while True:\n",
    "    prompt = builtins.input(\">>> \")\n",
    "    output=generate(prompt)\n",
    "    print(f\"Model output: {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
